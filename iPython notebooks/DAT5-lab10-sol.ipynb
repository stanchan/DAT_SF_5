{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# GA Data Science 5 (DAT5) - Lab 10\n",
      "\n",
      "### Text Mining via TF-IDF\n",
      "\n",
      "Rob Hall\n",
      "\n",
      "\n",
      "\n",
      "### Last Time:\n",
      "\n",
      "- #### K-Means Clustering\n",
      "\n",
      "#### Questions?\n",
      "\n",
      "\n",
      "### Today\n",
      "\n",
      "1. Basic text mining via TF-IDF\n",
      "2. MapReduce (not in this notebook)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "1. Introduction to the Vector Space Model (VSM)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In information retrieval or text mining, the term frequency \u2013 inverse document frequency also called tf-idf, is a well know method to evaluate how important is a word in a document. Tf-idf are also a very interesting way to convert the textual representation of information into a Vector Space Model (VSM), or into sparse features. We\u2019ll discuss more about it later. First, let\u2019s try to understand what tf-idf and the VSM are."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1.1 Going to the Vector Space"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first step in modeling the document into a vector space is to create a dictionary of terms present in documents. To do that, you can simple select all terms from the document and convert it to a dimension in the vector space, but we know that there are some kind of words (stop words) that are present in almost all documents, and what we\u2019re doing is extracting important features from documents, features do identify them among other similar documents, so using terms like \u201cthe, is, at, on\u201d, etc.. isn\u2019t going to help us, so in the information extraction, we\u2019ll just ignore them.\n",
      "\n",
      "Let\u2019s take the documents below to define our (extremely basic) document space:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Train Document Set:\n",
      "\n",
      "d1: The sky is blue.\n",
      "d2: The sun is bright.\n",
      "\n",
      "Test Document Set:\n",
      "\n",
      "d3: The sun in the sky is bright.\n",
      "d4: We can see the shining sun, the bright sun."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, what we have to do is to create a index vocabulary (dictionary) of the words of the train document set, using the documents d1 and d2 from the document set, we\u2019ll have the following index vocabulary denoted as {E}(t) where the t is the term:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/1-Eoft.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the terms like \u201cis\u201d and \u201cthe\u201d were ignored as cited before. Now that we have an index vocabulary, we can convert the test document set into a vector space where each term of the vector is indexed as our index vocabulary, so the first term of the vector represents the \u201cblue\u201d term of our vocabulary, the second represents \u201csun\u201d and so on. Now, we\u2019re going to use the term-frequency to represent each term in our vector space; the term-frequency is nothing more than a measure of how many times the terms present in our vocabulary {E}(t) are present in the documents d3 or d4, we define the term-frequency as a couting function:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/2.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "where the {fr}(x, t) is a simple function defined as:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/3.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, what the tf(t,d) returns is how many times is the term t is present in the document d. An example of this, could be tf(``sun'', d4) = 2 since we have only two occurrences of the term \u201csun\u201d in the document d4. Now you understood how the term-frequency works, we can go on into the creation of the document vector, which is represented by:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/4.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each dimension of the document vector is represented by the term of the vocabulary, for example, the {tf}(t_1,d_2) represents the frequency-term of the term 1 or t_1 (which is our \u201cblue\u201d term of the vocabulary) in the document d_2.\n",
      "\n",
      "Let\u2019s now show a concrete example of how the documents d_3 and d_4 are represented as vectors:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/5.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Which evaluates to:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/6.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you can see, since the documents d_3 and d_4 are:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "d3: The sun in the sky is bright.\n",
      "d4: We can see the shining sun, the bright sun."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The resulting vector v_{d_3} shows that we have, in order, 0 occurrences of the term \u201cblue\u201d, 1 occurrence of the term \u201csun\u201d, and so on. In the v_{d_3}, we have 0 occurences of the term \u201cblue\u201d, 2 occurrences of the term \u201csun\u201d, etc.\n",
      "\n",
      "But wait, since we have a collection of documents, now represented by vectors, we can represent them as a matrix with |D| \\times F shape, where |D| is the cardinality of the document space, or how many documents we have and the F is the number of features, in our case represented by the vocabulary size. An example of the matrix representation of the vectors described above is:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/7.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As you may have noted, these matrices representing the term frequencies tend to be very sparse (with majority of terms zeroed), and that\u2019s why you\u2019ll see a common representation of these matrix as sparse matrices."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "1.2 Now, in Python"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer(stop_words=\"english\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_set = (\"The sky is blue.\",\n",
      "             \"The sun is bright.\")\n",
      "test_set = (\"The sun in the sky is bright.\",\n",
      "            \"We can see the shining sun, the bright sun.\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print vectorizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CountVectorizer(analyzer=word, binary=False, charset=None, charset_error=None,\n",
        "        decode_error=strict, dtype=<type 'numpy.int64'>, encoding=utf-8,\n",
        "        input=content, lowercase=True, max_df=1.0, max_features=None,\n",
        "        min_df=1, ngram_range=(1, 1), preprocessor=None,\n",
        "        stop_words=english, strip_accents=None,\n",
        "        token_pattern=(?u)\\b\\w\\w+\\b, tokenizer=None, vocabulary=None)\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer.fit_transform(train_set)\n",
      "print vectorizer.vocabulary_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{u'blue': 0, u'sun': 3, u'bright': 1, u'sky': 2}\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sparse_matrix = vectorizer.transform(test_set)\n",
      "\n",
      "print vectorizer.vocabulary_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{u'blue': 0, u'sun': 3, u'bright': 1, u'sky': 2}\n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sparse_matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  (0, 1)\t1\n",
        "  (0, 2)\t1\n",
        "  (0, 3)\t1\n",
        "  (1, 1)\t1\n",
        "  (1, 3)\t2\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that the sparse matrix created called smatrix is a Scipy sparse matrix with elements stored in a Coordinate format. But we can convert it into a dense format:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sparse_matrix.todense()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "matrix([[0, 1, 1, 1],\n",
        "        [0, 1, 0, 2]])"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each row in the above represents a document in our test set (d3 and d4). So, that gives us a (very basic) matrix of term frequencies in our documents. We have mapped our text documents into a vector space!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "2. Vector Normalization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In Section 1 above, we learned how to use the term-frequency to represent textual information in the vector space. However, the main problem with the term-frequency approach is that it scales up frequent terms and scales down rare terms which are empirically more informative than the high frequency terms. The basic intuition is that a term that occurs frequently in many documents is not a good discriminator. The important question here is: why would you, in a classification problem for instance, emphasize a term which is almost present in the entire corpus of all your documents?\n",
      "\n",
      "The tf-idf weight solves this problem. What tf-idf gives is how important is a word to a document in a collection. That\u2019s why tf-idf incorporates local and global parameters; it takes in consideration not only the isolated term but also the term within the document collection. What tf-idf then does to solve that problem, is to scale down the frequent terms while scaling up the rare terms. A term that occurs 10 times more than another isn\u2019t 10 times more important than it. That\u2019s why tf-idf uses the logarithmic scale to do that.\n",
      "\n",
      "Let\u2019s go back to our definition of the {tf}(t,d) which is actually the term count of the term t in the document d. The use of this simple term frequency could lead us to problems like keyword spamming, which is when we have a repeated term in a document with the purpose of improving its ranking on an IR (Information Retrieval) system or even create a bias towards long documents, making them look more important than they are just because of the high frequency of the term in the document.\n",
      "\n",
      "To overcome this problem, the term frequency {tf}(t,d) of a document on a vector space is usually also normalized. In this Section 2, we will learn how to normalize this vector."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Suppose we are going to normalize the term-frequency vector {v_{d_4}} that we have calculated in the first part of this tutorial. The document d4 from the first part of this tutorial had this textual representation:"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "d4: We can see the shining sun, the bright sun."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And the vector space representation using the non-normalized term-frequency of that document was:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/8.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To normalize the vector, is the same as calculating the Unit Vector of the vector, and they are denoted using the \u201chat\u201d notation: hat{v}. The definition of the unit vector hat{v} of a vector {v} is:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/9.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Where the hat{v} is the unit vector, or the normalized vector, the {v} is the vector going to be normalized and the \\|{v}\\|_p is the norm (magnitude, length) of the vector {v} in the L^p space."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The unit vector is actually nothing more than a normalized version of the vector, is a vector which the length is 1."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/10.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But the important question here is how the length of the vector is calculated and to understand this, you must understand the motivation of the L^p spaces, also called Lebesgue spaces."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2.1 Lebesgue Spaces"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/11.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Usually, the length of a vector {u} = (u_1, u_2, u_3, ..., u_n) is calculated using the Euclidean norm \u2013 a norm is a function that assigns a strictly positive length or size to all vectors in a vector space -, which is defined by:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/12.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/13.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But this isn\u2019t the only way to define length, and that\u2019s why you see (sometimes) a number p together with the norm notation, like in \\|{u}\\|_p. That\u2019s because it could be generalized as:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/14.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "and simplified as:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/15.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So when you read about a L2-norm, you\u2019re reading about the Euclidean norm, a norm with p=2, the most common norm used to measure the length of a vector, typically called \u201cmagnitude\u201d; actually, when you have an unqualified length measure (without the p number), you have the L2-norm (Euclidean norm).\n",
      "\n",
      "When you read about a L1-norm, you\u2019re reading about the norm with p=1, defined as:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/16.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Which is nothing more than a simple sum of the components of the vector, also known as Taxicab distance, also called Manhattan distance."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/manhattan_distance.svg\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Taxicab geometry versus Euclidean distance: In taxicab geometry all three pictured lines have the same length (12) for the same route. In Euclidean geometry, the green line has length 6 x sqrt{2} = 8.48, and is the unique shortest path."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that you can also use any norm to normalize the vector, but we\u2019re going to use the most common norm, the L2-Norm, which is also the default in the 0.9 release of the scikit-learn. You can also find papers comparing the performance of the two approaches among other methods to normalize the document vector, actually you can use any other method, but you have to be concise, once you\u2019ve used a norm, you have to use it for the whole process directly involving the norm (a unit vector that used a L1-norm isn\u2019t going to have the length 1 if you\u2019re going to take its L2-norm later)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2.2 Back to Vector Normalization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we know what the vector normalization process is, we can try a concrete example, the process of using the L2-norm (we\u2019ll use the right terms now) to normalize our vector {v_{d_4}} = (0,2,1,0) in order to get its unit vector hat{v_{d_4}}. To do that, we\u2019ll simple plug it into the definition of the unit vector to evaluate it:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/17.jpg\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And that is it ! Our normalized vector hat{v_{d_4}} has now a L2-norm \\|hat{v_{d_4}}\\|_2 = 1.0.\n",
      "\n",
      "Note that here we have normalized our term frequency document vector, but later we\u2019re going to do that after the calculation of the tf-idf."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2.3 The Term Frequency - Inverse Document Frequency (TF-IDF) Weight"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "Train Document Set:\n",
      "\n",
      "d1: The sky is blue.\n",
      "d2: The sun is bright.\n",
      "\n",
      "Test Document Set:\n",
      "\n",
      "d3: The sun in the sky is bright.\n",
      "d4: We can see the shining sun, the bright sun."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our document space can be defined then as D = { d_1, d_2, ..., d_n} where n is the number of documents in our corpus, and in our case as D_{train} = {d_1, d_2} and D_{test} = {d_3, d_4}. The cardinality of our document space is defined by |{D_{train}}| = 2 and |{D_{test}}| = 2, since we have only 2 two documents for training and testing, but they obviously don\u2019t need to have the same cardinality.\n",
      "\n",
      "Let\u2019s see now, how idf (inverse document frequency) is then defined:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/18.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "where |{d : t in d}| is the number of documents where the term t appears, when the term-frequency function satisfies {tf}(t,d) <> 0, we\u2019re only adding 1 into the formula to avoid zero-division.\n",
      "\n",
      "The formula for the tf-idf is then:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/19.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This formula has an important consequence: a high weight of the tf-idf calculation is reached when we have a high term frequency (tf) in the given document (local parameter) and a low document frequency of the term in the whole collection (global parameter).\n",
      "\n",
      "Now, let\u2019s calculate the idf for each feature present in the feature matrix with the term frequency we have calculated in the first section above:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/20.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since we have 4 features, we have to calculate {idf}(t_1), {idf}(t_2), {idf}(t_3), {idf}(t_4):"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/21a.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/22.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/23.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/24.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These idf weights can be represented by a vector as:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/25.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have our matrix with the term frequency (M_{train}) and the vector representing the idf for each feature of our matrix ({idf_{train}}), we can calculate our tf-idf weights. What we have to do is a simple multiplication of each column of the matrix M_{train} with the respective {idf_{train}} vector dimension. To do that, we can create a square diagonal matrix called M_{idf} with both the vertical and horizontal dimensions equal to the vector {idf_{train}} dimension:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/26.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "and then multiply it to the term frequency matrix, so the final result can be defined then as:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/27.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Please note that the matrix multiplication isn\u2019t commutative, the result of A x B will be different than the result of the B x A, and this is why the M_{idf} is on the right side of the multiplication, to accomplish the desired effect of multiplying each idf value to its corresponding feature:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/28.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let\u2019s see now a concrete example of this multiplication:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/29.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And finally, we can apply our L2 normalization process to the M_{tf-idf} matrix. Please note that this normalization is \u201crow-wise\u201d because we\u2019re going to handle each row of the matrix as a separated vector to be normalized, and not the matrix as a whole:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"files/assets/31.png\" /><img src=\"files/assets/30.png\" />"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And that is our pretty normalized tf-idf weight of our testing document set, which is actually a collection of unit vectors. If we take the L2-norm of each row of the matrix, we\u2019ll see that they all have a L2-norm of 1."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "2.4 Now in Python"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The first step is to create our training and testing document set and computing the term frequency matrix:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "train_set = (\"The sky is blue.\", \"The sun is bright.\")\n",
      "test_set = (\"The sun in the sky is bright.\",\n",
      "\"We can see the shining sun, the bright sun.\")\n",
      "\n",
      "count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
      "count_vectorizer.fit_transform(train_set)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "<2x4 sparse matrix of type '<type 'numpy.int64'>'\n",
        "\twith 4 stored elements in Compressed Sparse Column format>"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Vocabulary:\", count_vectorizer.vocabulary_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Vocabulary: {u'blue': 0, u'sun': 3, u'bright': 1, u'sky': 2}\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "freq_term_matrix = count_vectorizer.transform(test_set)\n",
      "print freq_term_matrix.todense()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[1 0 1 1]\n",
        " [1 1 0 2]]\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have the frequency term matrix (called freq_term_matrix), we can instantiate the TfidfTransformer, which is going to be responsible to calculate the tf-idf weights for our term frequency matrix:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "\n",
      "tfidf = TfidfTransformer(norm=\"l2\")\n",
      "tfidf.fit(freq_term_matrix)\n",
      "\n",
      "print \"IDF:\", tfidf.idf_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "IDF: [ 2.09861229  1.          1.40546511  1.        ]\n"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that we\u2019ve specified the norm as L2. This is optional (actually the default is L2-norm), but we\u2019ve added the parameter to make it explicit that it\u2019s going to use the L2-norm. Also note that we can see the calculated idf weight by accessing the internal attribute called idf_. Now that fit() method has calculated the idf for the matrix, let\u2019s transform the freq_term_matrix to the tf-idf weight matrix:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tf_idf_matrix = tfidf.transform(freq_term_matrix)\n",
      "print tf_idf_matrix.todense()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 0.          0.50154891  0.70490949  0.50154891]\n",
        " [ 0.          0.4472136   0.          0.89442719]]\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}